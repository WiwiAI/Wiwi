from llama_cpp import Llama 
import torch

print("CUDA available:", torch.cuda.is_available())
